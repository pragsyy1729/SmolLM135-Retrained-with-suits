{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c2oCgBbvVM4o"
      },
      "source": [
        "# SmolLM2-135M Training from Scratch\n",
        "\n",
        "This notebook implements the SmolLM2-135M architecture (Llama-based) and trains it from scratch.\n",
        "\n",
        "## Model Architecture\n",
        "- **Model Type**: LlamaForCausalLM (decoder-only transformer)\n",
        "- **Parameters**: ~134.5M\n",
        "- **Key Features**: RMSNorm, RoPE, Grouped Query Attention (GQA), SwiGLU MLP\n",
        "\n",
        "## Training Plan\n",
        "- Train for 5000 steps with checkpointing every 500 steps\n",
        "- Generate text with fixed prompts every 500 steps\n",
        "- Stop and resume from checkpoint for 50 more steps"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hPqbBdDfVM4o"
      },
      "source": [
        "## 1. Setup and Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vLBhGZ8zVM4p",
        "outputId": "3a33d1ac-d8b4-4376-be76-7ddcb6ee6322"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.12/dist-packages (0.12.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken) (2025.11.3)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.12/dist-packages (from tiktoken) (2.32.4)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken) (2026.1.4)\n"
          ]
        }
      ],
      "source": [
        "!pip install tiktoken"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hLj5HkQ1VM4p",
        "outputId": "76a87236-c19e-4091-9a8e-0dba625ae326"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PyTorch version: 2.9.0+cu126\n",
            "CUDA available: True\n",
            "CUDA device: NVIDIA A100-SXM4-40GB\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import math\n",
        "import time\n",
        "from dataclasses import dataclass\n",
        "from typing import Optional, Tuple\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "import tiktoken\n",
        "\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IWxZMZp3VM4p"
      },
      "source": [
        "## 2. Model Configuration\n",
        "\n",
        "SmolLM2-135M configuration from official training recipe."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "MoOwJWutVM4p"
      },
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class SmolLM2Config:\n",
        "    \"\"\"SmolLM2-135M Configuration\"\"\"\n",
        "    vocab_size: int = 50304\n",
        "    hidden_size: int = 576\n",
        "    intermediate_size: int = 1536\n",
        "    num_hidden_layers: int = 30\n",
        "    num_attention_heads: int = 9\n",
        "    num_key_value_heads: int = 3\n",
        "    max_position_embeddings: int = 2048\n",
        "    rms_norm_eps: float = 1e-5\n",
        "    rope_theta: float = 10000.0\n",
        "    hidden_act: str = \"silu\"\n",
        "    initializer_range: float = 0.041666666666666664\n",
        "    tie_word_embeddings: bool = True\n",
        "    bos_token_id: int = 0\n",
        "    eos_token_id: int = 0\n",
        "\n",
        "    @property\n",
        "    def head_dim(self) -> int:\n",
        "        return self.hidden_size // self.num_attention_heads"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VwQRUx_DVM4p"
      },
      "source": [
        "## 3. Model Architecture Components\n",
        "\n",
        "### 3.1 RMSNorm (Root Mean Square Layer Normalization)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "CZDXOREwVM4p"
      },
      "outputs": [],
      "source": [
        "class RMSNorm(nn.Module):\n",
        "    \"\"\"Root Mean Square Layer Normalization\n",
        "\n",
        "    Unlike LayerNorm, RMSNorm doesn't center the activations (no mean subtraction).\n",
        "    This makes it more efficient while maintaining similar performance.\n",
        "\n",
        "    Formula: x * weight / sqrt(mean(x^2) + eps)\n",
        "    \"\"\"\n",
        "    def __init__(self, hidden_size: int, eps: float = 1e-5):\n",
        "        super().__init__()\n",
        "        self.weight = nn.Parameter(torch.ones(hidden_size))\n",
        "        self.eps = eps\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        # Calculate RMS\n",
        "        rms = torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)\n",
        "        return x * rms * self.weight"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sDTpq1NjVM4q"
      },
      "source": [
        "### 3.2 Rotary Position Embedding (RoPE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "qU-qmMe5VM4q"
      },
      "outputs": [],
      "source": [
        "class RotaryEmbedding(nn.Module):\n",
        "    \"\"\"Rotary Position Embedding (RoPE)\n",
        "\n",
        "    RoPE encodes position information by rotating the query and key vectors.\n",
        "    This allows the model to learn relative positions naturally.\n",
        "\n",
        "    Key insight: The dot product of rotated vectors depends on their relative position.\n",
        "    \"\"\"\n",
        "    def __init__(self, dim: int, max_position_embeddings: int = 2048, theta: float = 10000.0):\n",
        "        super().__init__()\n",
        "        self.dim = dim\n",
        "        self.max_position_embeddings = max_position_embeddings\n",
        "        self.theta = theta\n",
        "\n",
        "        # Compute inverse frequencies\n",
        "        inv_freq = 1.0 / (self.theta ** (torch.arange(0, self.dim, 2, dtype=torch.float32) / self.dim))\n",
        "        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n",
        "\n",
        "        # Precompute cos and sin cache\n",
        "        self._set_cos_sin_cache(max_position_embeddings)\n",
        "\n",
        "    def _set_cos_sin_cache(self, seq_len: int):\n",
        "        self.max_seq_len_cached = seq_len\n",
        "        t = torch.arange(seq_len, dtype=torch.float32)\n",
        "        freqs = torch.outer(t, self.inv_freq)\n",
        "        emb = torch.cat((freqs, freqs), dim=-1)\n",
        "        self.register_buffer(\"cos_cached\", emb.cos(), persistent=False)\n",
        "        self.register_buffer(\"sin_cached\", emb.sin(), persistent=False)\n",
        "\n",
        "    def forward(self, x: torch.Tensor, seq_len: int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "        if seq_len > self.max_seq_len_cached:\n",
        "            self._set_cos_sin_cache(seq_len)\n",
        "        return (\n",
        "            self.cos_cached[:seq_len].to(x.dtype),\n",
        "            self.sin_cached[:seq_len].to(x.dtype)\n",
        "        )\n",
        "\n",
        "\n",
        "def rotate_half(x: torch.Tensor) -> torch.Tensor:\n",
        "    \"\"\"Rotate half the hidden dims of the input.\"\"\"\n",
        "    x1 = x[..., : x.shape[-1] // 2]\n",
        "    x2 = x[..., x.shape[-1] // 2 :]\n",
        "    return torch.cat((-x2, x1), dim=-1)\n",
        "\n",
        "\n",
        "def apply_rotary_pos_emb(q: torch.Tensor, k: torch.Tensor, cos: torch.Tensor, sin: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "    \"\"\"Apply rotary position embedding to query and key tensors.\"\"\"\n",
        "    # cos, sin: [seq_len, dim] -> [1, 1, seq_len, dim]\n",
        "    cos = cos.unsqueeze(0).unsqueeze(0)\n",
        "    sin = sin.unsqueeze(0).unsqueeze(0)\n",
        "\n",
        "    q_embed = (q * cos) + (rotate_half(q) * sin)\n",
        "    k_embed = (k * cos) + (rotate_half(k) * sin)\n",
        "    return q_embed, k_embed"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cu_4tGX7VM4q"
      },
      "source": [
        "### 3.3 Grouped Query Attention (GQA)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "itkQxie2VM4q"
      },
      "outputs": [],
      "source": [
        "class GroupedQueryAttention(nn.Module):\n",
        "    \"\"\"Grouped Query Attention (GQA)\n",
        "\n",
        "    GQA is a memory-efficient attention variant where multiple query heads\n",
        "    share the same key-value heads. SmolLM2-135M uses 9 query heads with 3 KV heads,\n",
        "    so each KV head is shared by 3 query heads.\n",
        "\n",
        "    Benefits:\n",
        "    - Reduces memory bandwidth during inference (smaller KV cache)\n",
        "    - Maintains model quality compared to full MHA\n",
        "    \"\"\"\n",
        "    def __init__(self, config: SmolLM2Config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.hidden_size = config.hidden_size\n",
        "        self.num_heads = config.num_attention_heads\n",
        "        self.num_kv_heads = config.num_key_value_heads\n",
        "        self.head_dim = config.head_dim\n",
        "        self.num_kv_groups = self.num_heads // self.num_kv_heads  # 9 // 3 = 3\n",
        "\n",
        "        # Q projection: hidden_size -> num_heads * head_dim\n",
        "        self.q_proj = nn.Linear(self.hidden_size, self.num_heads * self.head_dim, bias=False)\n",
        "        # K, V projections: hidden_size -> num_kv_heads * head_dim\n",
        "        self.k_proj = nn.Linear(self.hidden_size, self.num_kv_heads * self.head_dim, bias=False)\n",
        "        self.v_proj = nn.Linear(self.hidden_size, self.num_kv_heads * self.head_dim, bias=False)\n",
        "        # Output projection\n",
        "        self.o_proj = nn.Linear(self.num_heads * self.head_dim, self.hidden_size, bias=False)\n",
        "\n",
        "        # Rotary embeddings\n",
        "        self.rotary_emb = RotaryEmbedding(\n",
        "            self.head_dim,\n",
        "            max_position_embeddings=config.max_position_embeddings,\n",
        "            theta=config.rope_theta\n",
        "        )\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        B, T, C = x.size()\n",
        "\n",
        "        # Project to Q, K, V\n",
        "        q = self.q_proj(x)  # (B, T, num_heads * head_dim)\n",
        "        k = self.k_proj(x)  # (B, T, num_kv_heads * head_dim)\n",
        "        v = self.v_proj(x)  # (B, T, num_kv_heads * head_dim)\n",
        "\n",
        "        # Reshape for multi-head attention\n",
        "        q = q.view(B, T, self.num_heads, self.head_dim).transpose(1, 2)  # (B, num_heads, T, head_dim)\n",
        "        k = k.view(B, T, self.num_kv_heads, self.head_dim).transpose(1, 2)  # (B, num_kv_heads, T, head_dim)\n",
        "        v = v.view(B, T, self.num_kv_heads, self.head_dim).transpose(1, 2)  # (B, num_kv_heads, T, head_dim)\n",
        "\n",
        "        # Apply rotary position embeddings\n",
        "        cos, sin = self.rotary_emb(x, T)\n",
        "        q, k = apply_rotary_pos_emb(q, k, cos, sin)\n",
        "\n",
        "        # Repeat K, V for grouped query attention\n",
        "        # Each KV head is used by num_kv_groups query heads\n",
        "        k = k.repeat_interleave(self.num_kv_groups, dim=1)  # (B, num_heads, T, head_dim)\n",
        "        v = v.repeat_interleave(self.num_kv_groups, dim=1)  # (B, num_heads, T, head_dim)\n",
        "\n",
        "        # Flash Attention using scaled_dot_product_attention\n",
        "        y = F.scaled_dot_product_attention(q, k, v, is_causal=True)\n",
        "\n",
        "        # Reshape back\n",
        "        y = y.transpose(1, 2).contiguous().view(B, T, self.hidden_size)\n",
        "\n",
        "        # Output projection\n",
        "        y = self.o_proj(y)\n",
        "        return y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EuKx71-GVM4q"
      },
      "source": [
        "### 3.4 SwiGLU MLP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "UnIq4LgzVM4q"
      },
      "outputs": [],
      "source": [
        "class SwiGLUMLP(nn.Module):\n",
        "    \"\"\"SwiGLU Feed-Forward Network\n",
        "\n",
        "    SwiGLU is a variant of GLU (Gated Linear Unit) that uses SiLU activation.\n",
        "    It has been shown to improve model quality over standard GELU MLPs.\n",
        "\n",
        "    Formula: output = down_proj(silu(gate_proj(x)) * up_proj(x))\n",
        "\n",
        "    Note: This uses 3 linear layers instead of 2, but intermediate_size is\n",
        "    adjusted so total parameters are similar.\n",
        "    \"\"\"\n",
        "    def __init__(self, config: SmolLM2Config):\n",
        "        super().__init__()\n",
        "        self.hidden_size = config.hidden_size\n",
        "        self.intermediate_size = config.intermediate_size\n",
        "\n",
        "        # Three projections for SwiGLU\n",
        "        self.gate_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)\n",
        "        self.up_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)\n",
        "        self.down_proj = nn.Linear(self.intermediate_size, self.hidden_size, bias=False)\n",
        "        self.act_fn = nn.SiLU()\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        # SwiGLU: silu(gate) * up, then down\n",
        "        return self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "grykxWWaVM4q"
      },
      "source": [
        "### 3.5 Transformer Block"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "8b586rYcVM4q"
      },
      "outputs": [],
      "source": [
        "class SmolLM2Block(nn.Module):\n",
        "    \"\"\"SmolLM2 Transformer Block\n",
        "\n",
        "    Pre-norm architecture with:\n",
        "    - RMSNorm before attention and MLP\n",
        "    - Residual connections after attention and MLP\n",
        "    \"\"\"\n",
        "    def __init__(self, config: SmolLM2Config):\n",
        "        super().__init__()\n",
        "        self.input_layernorm = RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n",
        "        self.self_attn = GroupedQueryAttention(config)\n",
        "        self.post_attention_layernorm = RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n",
        "        self.mlp = SwiGLUMLP(config)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        # Pre-norm + Attention + Residual\n",
        "        x = x + self.self_attn(self.input_layernorm(x))\n",
        "        # Pre-norm + MLP + Residual\n",
        "        x = x + self.mlp(self.post_attention_layernorm(x))\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p6Dvm-wmVM4r"
      },
      "source": [
        "### 3.6 Complete SmolLM2 Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "Y-PvPl-fVM4r"
      },
      "outputs": [],
      "source": [
        "class SmolLM2(nn.Module):\n",
        "    \"\"\"SmolLM2-135M: A Llama-based decoder-only transformer\n",
        "\n",
        "    Architecture:\n",
        "    - Token embeddings (no position embeddings - RoPE is applied in attention)\n",
        "    - 30 transformer blocks with GQA and SwiGLU\n",
        "    - Final RMSNorm\n",
        "    - Language model head (tied with token embeddings)\n",
        "    \"\"\"\n",
        "    def __init__(self, config: SmolLM2Config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "\n",
        "        # Token embeddings (no position embeddings - using RoPE)\n",
        "        self.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size)\n",
        "\n",
        "        # Transformer blocks\n",
        "        self.layers = nn.ModuleList([SmolLM2Block(config) for _ in range(config.num_hidden_layers)])\n",
        "\n",
        "        # Final layer norm\n",
        "        self.norm = RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n",
        "\n",
        "        # Language model head\n",
        "        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n",
        "\n",
        "        # Weight tying\n",
        "        if config.tie_word_embeddings:\n",
        "            self.lm_head.weight = self.embed_tokens.weight\n",
        "\n",
        "        # Initialize weights\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        \"\"\"Initialize weights with the specified initializer range\"\"\"\n",
        "        if isinstance(module, nn.Linear):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=self.config.initializer_range)\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=self.config.initializer_range)\n",
        "\n",
        "    def forward(self, input_ids: torch.Tensor, targets: Optional[torch.Tensor] = None) -> Tuple[torch.Tensor, Optional[torch.Tensor]]:\n",
        "        B, T = input_ids.size()\n",
        "        assert T <= self.config.max_position_embeddings, \\\n",
        "            f\"Sequence length {T} exceeds max position embeddings {self.config.max_position_embeddings}\"\n",
        "\n",
        "        # Token embeddings\n",
        "        x = self.embed_tokens(input_ids)\n",
        "\n",
        "        # Forward through transformer blocks\n",
        "        for layer in self.layers:\n",
        "            x = layer(x)\n",
        "\n",
        "        # Final norm and LM head\n",
        "        x = self.norm(x)\n",
        "        logits = self.lm_head(x)\n",
        "\n",
        "        # Calculate loss if targets provided\n",
        "        loss = None\n",
        "        if targets is not None:\n",
        "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
        "\n",
        "        return logits, loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FwODmUWyVM4r"
      },
      "source": [
        "## 4. Parameter Count Verification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uqtxf22bVM4r",
        "outputId": "b8a1f6a6-3ed1-4ff9-b492-2fd3f94f5d82"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==================================================\n",
            "PARAMETER COUNT BY COMPONENT\n",
            "==================================================\n",
            "Embedding           :      28,975,104 (28.98M)\n",
            "Normalization       :          35,136 (0.04M)\n",
            "Attention           :      26,542,080 (26.54M)\n",
            "MLP                 :      79,626,240 (79.63M)\n",
            "--------------------------------------------------\n",
            "TOTAL               :     135,178,560 (135.18M)\n",
            "==================================================\n",
            "\n",
            "Expected: ~134,515,008 parameters\n",
            "Actual:   135,178,560 parameters\n",
            "Match: No\n"
          ]
        }
      ],
      "source": [
        "def count_parameters(model):\n",
        "    \"\"\"Count and display model parameters by component\"\"\"\n",
        "    total = 0\n",
        "    param_counts = {}\n",
        "\n",
        "    for name, param in model.named_parameters():\n",
        "        total += param.numel()\n",
        "\n",
        "        # Group by component\n",
        "        if 'embed_tokens' in name:\n",
        "            key = 'Embedding'\n",
        "        elif 'self_attn' in name:\n",
        "            key = 'Attention'\n",
        "        elif 'mlp' in name:\n",
        "            key = 'MLP'\n",
        "        elif 'norm' in name or 'layernorm' in name:\n",
        "            key = 'Normalization'\n",
        "        elif 'lm_head' in name:\n",
        "            key = 'LM Head (tied)'\n",
        "        else:\n",
        "            key = 'Other'\n",
        "\n",
        "        param_counts[key] = param_counts.get(key, 0) + param.numel()\n",
        "\n",
        "    print(\"=\" * 50)\n",
        "    print(\"PARAMETER COUNT BY COMPONENT\")\n",
        "    print(\"=\" * 50)\n",
        "    for key, count in param_counts.items():\n",
        "        print(f\"{key:20s}: {count:>15,} ({count/1e6:.2f}M)\")\n",
        "    print(\"-\" * 50)\n",
        "    print(f\"{'TOTAL':20s}: {total:>15,} ({total/1e6:.2f}M)\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    return total\n",
        "\n",
        "# Create model and count parameters\n",
        "config = SmolLM2Config()\n",
        "model = SmolLM2(config)\n",
        "total_params = count_parameters(model)\n",
        "\n",
        "# Verify against expected ~134.5M\n",
        "print(f\"\\nExpected: ~134,515,008 parameters\")\n",
        "print(f\"Actual:   {total_params:,} parameters\")\n",
        "print(f\"Match: {'Yes' if abs(total_params - 134515008) < 1000 else 'No'}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ykupg8MQVM4r"
      },
      "source": [
        "## 5. Data Loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "IZ4G4fi_VM4r"
      },
      "outputs": [],
      "source": [
        "class DataLoaderLite:\n",
        "    \"\"\"Lightweight data loader for text tokenization and batching\"\"\"\n",
        "\n",
        "    def __init__(self, B: int, T: int, data_path: str = 'input.txt'):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            B: Batch size\n",
        "            T: Sequence length\n",
        "            data_path: Path to text file\n",
        "        \"\"\"\n",
        "        self.B = B\n",
        "        self.T = T\n",
        "\n",
        "        # Load and tokenize text\n",
        "        with open(data_path, 'r', encoding='utf-8') as f:\n",
        "            text = f.read()\n",
        "\n",
        "        enc = tiktoken.get_encoding('gpt2')\n",
        "        tokens = enc.encode(text)\n",
        "        self.tokens = torch.tensor(tokens)\n",
        "\n",
        "        print(f'Loaded {len(self.tokens):,} tokens')\n",
        "        print(f'1 epoch = {len(self.tokens) // (B * T):,} batches')\n",
        "\n",
        "        self.current_position = 0\n",
        "\n",
        "    def next_batch(self):\n",
        "        \"\"\"Get next batch of data\"\"\"\n",
        "        B, T = self.B, self.T\n",
        "        buf = self.tokens[self.current_position : self.current_position + B * T + 1]\n",
        "        x = buf[:-1].view(B, T)  # Inputs\n",
        "        y = buf[1:].view(B, T)   # Targets (shifted by 1)\n",
        "\n",
        "        # Advance position\n",
        "        self.current_position += B * T\n",
        "\n",
        "        # Reset if at end of data\n",
        "        if self.current_position + (B * T + 1) > len(self.tokens):\n",
        "            self.current_position = 0\n",
        "\n",
        "        return x, y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mczymUUXVM4r"
      },
      "source": [
        "## 6. Text Generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "YxQUrufnVM4r"
      },
      "outputs": [],
      "source": [
        "# Fixed prompts for consistent evaluation\n",
        "FIXED_PROMPTS = [\n",
        "    \"Once upon a time\",\n",
        "    \"The meaning of life is\",\n",
        "    \"In a galaxy far away\",\n",
        "]\n",
        "\n",
        "def generate_text(model, prompt: str, max_new_tokens: int = 50, temperature: float = 0.8, top_k: int = 50, device: str = 'cuda'):\n",
        "    \"\"\"Generate text from a prompt using the trained model\"\"\"\n",
        "    model.eval()\n",
        "    enc = tiktoken.get_encoding('gpt2')\n",
        "\n",
        "    # Encode prompt\n",
        "    tokens = enc.encode(prompt)\n",
        "    tokens = torch.tensor(tokens, dtype=torch.long, device=device).unsqueeze(0)\n",
        "\n",
        "    # Generate tokens\n",
        "    with torch.no_grad():\n",
        "        for _ in range(max_new_tokens):\n",
        "            # Crop to max position embeddings\n",
        "            idx_cond = tokens[:, -model.config.max_position_embeddings:]\n",
        "\n",
        "            # Get predictions\n",
        "            logits, _ = model(idx_cond)\n",
        "            logits = logits[:, -1, :] / temperature\n",
        "\n",
        "            # Top-k sampling\n",
        "            if top_k is not None:\n",
        "                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
        "                logits[logits < v[:, [-1]]] = float('-inf')\n",
        "\n",
        "            # Sample\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            next_token = torch.multinomial(probs, num_samples=1)\n",
        "            tokens = torch.cat([tokens, next_token], dim=1)\n",
        "\n",
        "    # Decode\n",
        "    generated = enc.decode(tokens[0].tolist())\n",
        "    model.train()\n",
        "    return generated\n",
        "\n",
        "\n",
        "def generate_samples(model, step: int, device: str = 'cuda'):\n",
        "    \"\"\"Generate samples from fixed prompts and print them\"\"\"\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"TEXT GENERATION AT STEP {step}\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    for i, prompt in enumerate(FIXED_PROMPTS):\n",
        "        generated = generate_text(model, prompt, max_new_tokens=50, device=device)\n",
        "        print(f\"\\nPrompt {i+1}: '{prompt}'\")\n",
        "        print(f\"Generated: {generated}\")\n",
        "\n",
        "    print(f\"{'='*60}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bfGGTNJ8VM4r"
      },
      "source": [
        "## 7. Training Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DzlYJ7xlVM4r",
        "outputId": "dbe83213-18e1-467a-a0b4-dea445af17ad"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
            "  _C._set_float32_matmul_precision(precision)\n"
          ]
        }
      ],
      "source": [
        "# Device setup\n",
        "device = 'cpu'\n",
        "if torch.cuda.is_available():\n",
        "    device = 'cuda'\n",
        "elif hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
        "    device = \"mps\"\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Set seeds for reproducibility\n",
        "torch.manual_seed(1337)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed(1337)\n",
        "\n",
        "# Speedup: TF32 precision\n",
        "torch.set_float32_matmul_precision('high')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ATMxSP4nVM4r",
        "outputId": "71286503-6094-4a0d-941e-8c47b545b74d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training for 5,000 steps\n",
            "Checkpoint interval: 500 steps\n",
            "Effective batch size: 16 (gradient accumulation: 4x)\n",
            "Sequence length: 1024\n",
            "Learning rate: 6.00e-05 -> 6.00e-04 -> 6.00e-05\n"
          ]
        }
      ],
      "source": [
        "# Training hyperparameters\n",
        "max_steps = 5000  # Total training steps\n",
        "checkpoint_interval = 500  # Checkpoint and generate every N steps\n",
        "\n",
        "# Batch sizes\n",
        "total_batch_size = 16  # Effective batch size\n",
        "micro_batch_size = 4   # Actual batch size per forward pass\n",
        "sequence_length = 1024  # Context window\n",
        "gradient_accumulation_steps = total_batch_size // micro_batch_size\n",
        "\n",
        "# Learning rate schedule (cosine with warmup)\n",
        "max_lr = 6e-4\n",
        "min_lr = max_lr * 0.1\n",
        "warmup_steps = 500\n",
        "\n",
        "# Optimizer\n",
        "weight_decay = 0.1\n",
        "grad_clip = 1.0\n",
        "\n",
        "# Checkpoint directory\n",
        "checkpoint_dir = 'checkpoints'\n",
        "os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "\n",
        "print(f\"Training for {max_steps:,} steps\")\n",
        "print(f\"Checkpoint interval: {checkpoint_interval} steps\")\n",
        "print(f\"Effective batch size: {total_batch_size} (gradient accumulation: {gradient_accumulation_steps}x)\")\n",
        "print(f\"Sequence length: {sequence_length}\")\n",
        "print(f\"Learning rate: {min_lr:.2e} -> {max_lr:.2e} -> {min_lr:.2e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "uIrvH428VM4r"
      },
      "outputs": [],
      "source": [
        "def get_lr(step: int) -> float:\n",
        "    \"\"\"Cosine learning rate schedule with warmup\"\"\"\n",
        "    # Warmup\n",
        "    if step < warmup_steps:\n",
        "        return max_lr * (step + 1) / warmup_steps\n",
        "    # Cosine decay\n",
        "    if step > max_steps:\n",
        "        return min_lr\n",
        "    decay_ratio = (step - warmup_steps) / (max_steps - warmup_steps)\n",
        "    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio))\n",
        "    return min_lr + coeff * (max_lr - min_lr)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q4kpLgeLVM4r"
      },
      "source": [
        "## 8. Model Initialization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HhoN2hgeVM4r",
        "outputId": "8813c96a-7a39-4588-f9e7-f3c17fd0a1a1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Compiling model with torch.compile...\n",
            "\n",
            "Model parameters: 135,178,560 (135.2M)\n",
            "Loaded 124,471 tokens\n",
            "1 epoch = 30 batches\n"
          ]
        }
      ],
      "source": [
        "# Create model\n",
        "config = SmolLM2Config()\n",
        "model = SmolLM2(config)\n",
        "model.to(device)\n",
        "\n",
        "# Speedup: torch.compile (only on Linux/CUDA)\n",
        "if device == 'cuda' and hasattr(torch, 'compile'):\n",
        "    print(\"Compiling model with torch.compile...\")\n",
        "    model = torch.compile(model)\n",
        "\n",
        "# Count parameters\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "print(f\"\\nModel parameters: {total_params:,} ({total_params/1e6:.1f}M)\")\n",
        "\n",
        "# Initialize data loader\n",
        "train_loader = DataLoaderLite(B=micro_batch_size, T=sequence_length)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qJueLuY1VM4r"
      },
      "source": [
        "## 9. Checkpoint Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "rBNGTNZ2VM4r"
      },
      "outputs": [],
      "source": [
        "def save_checkpoint(model, optimizer, step: int, loss: float, path: str):\n",
        "    \"\"\"Save training checkpoint\"\"\"\n",
        "    # Handle compiled model\n",
        "    model_to_save = model._orig_mod if hasattr(model, '_orig_mod') else model\n",
        "\n",
        "    checkpoint = {\n",
        "        'step': step,\n",
        "        'model_state_dict': model_to_save.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'loss': loss,\n",
        "        'config': config,\n",
        "    }\n",
        "    torch.save(checkpoint, path)\n",
        "    print(f\"Checkpoint saved: {path}\")\n",
        "\n",
        "\n",
        "def load_checkpoint(model, optimizer, path: str, device: str):\n",
        "    \"\"\"Load training checkpoint\"\"\"\n",
        "    checkpoint = torch.load(path, map_location=device, weights_only=False)\n",
        "\n",
        "    # Handle compiled model\n",
        "    model_to_load = model._orig_mod if hasattr(model, '_orig_mod') else model\n",
        "    model_to_load.load_state_dict(checkpoint['model_state_dict'])\n",
        "\n",
        "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "\n",
        "    step = checkpoint['step']\n",
        "    loss = checkpoint['loss']\n",
        "    print(f\"Checkpoint loaded: {path} (step {step}, loss {loss:.4f})\")\n",
        "\n",
        "    return step, loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MwBZU6QXVM4r"
      },
      "source": [
        "## 10. Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TjadREzUVM4r",
        "outputId": "698472ef-eacb-4b74-ffb0-ac3723d7a8fd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "STARTING TRAINING\n",
            "============================================================\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Initialize optimizer\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
        "optimizer = torch.optim.AdamW(\n",
        "    model.parameters(),\n",
        "    lr=max_lr,\n",
        "    betas=(0.9, 0.95),\n",
        "    eps=1e-8,\n",
        "    weight_decay=weight_decay,\n",
        "    fused=True if device == 'cuda' else False  # Speedup: fused AdamW\n",
        ")\n",
        "\n",
        "# Training history\n",
        "loss_history = []\n",
        "start_time = time.time()\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"STARTING TRAINING\")\n",
        "print(\"=\"*60 + \"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FoBWJopsVM4s",
        "outputId": "818dc2a4-04cc-4dd2-a587-42a1efc692e1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step     0/5000 | Loss: 11.1658 | LR: 1.20e-06 | 242 tok/s\n",
            "Step   100/5000 | Loss: 5.0414 | LR: 1.21e-04 | 19053 tok/s\n",
            "Step   200/5000 | Loss: 2.4453 | LR: 2.41e-04 | 31114 tok/s\n",
            "Step   300/5000 | Loss: 0.8739 | LR: 3.61e-04 | 39505 tok/s\n",
            "Step   400/5000 | Loss: 0.1271 | LR: 4.81e-04 | 45393 tok/s\n",
            "Checkpoint saved: checkpoints/checkpoint_step_500.pt\n",
            "\n",
            "============================================================\n",
            "TEXT GENERATION AT STEP 500\n",
            "============================================================\n",
            "\n",
            "Prompt 1: 'Once upon a time'\n",
            "Generated: Once upon a time redDoes some closer doing\n",
            "\n",
            " the Because out us so back,\n",
            " it.\n",
            "This listen a everything anymore.\n",
            "I can me right\n",
            "whopering from from tell.\n",
            "Does apparently there, likewww.\n",
            "[Ch can thinking Harvey\n",
            "\n",
            "Prompt 2: 'The meaning of life is'\n",
            "Generated: The meaning of life is... against RememberCan their love it respect\n",
            " human CEO habit we.\n",
            "O after man sure could?\n",
            "cause.\n",
            "I, line's after books.\n",
            "I'm not the facts\n",
            "we can pretend call a very gonna\n",
            "myx away\n",
            "\n",
            "Prompt 3: 'In a galaxy far away'\n",
            "Generated: In a galaxy far awayl\n",
            " bike many then trying school.\n",
            "Har you are.\n",
            "if current more.\n",
            "So you could her\n",
            "just talk like.\n",
            "Har you want\n",
            "to was them me\n",
            "in ever there.\n",
            "Harvey's Mike\n",
            "in thing\n",
            "============================================================\n",
            "\n",
            "Step   500/5000 | Loss: 0.0999 | LR: 6.00e-04 | 36219 tok/s\n",
            "Step   600/5000 | Loss: 0.0482 | LR: 5.99e-04 | 40022 tok/s\n",
            "Step   700/5000 | Loss: 0.0294 | LR: 5.97e-04 | 43278 tok/s\n",
            "Step   800/5000 | Loss: 0.0307 | LR: 5.94e-04 | 46115 tok/s\n",
            "Step   900/5000 | Loss: 0.0262 | LR: 5.90e-04 | 48584 tok/s\n",
            "Checkpoint saved: checkpoints/checkpoint_step_1000.pt\n",
            "\n",
            "============================================================\n",
            "TEXT GENERATION AT STEP 1000\n",
            "============================================================\n",
            "\n",
            "Prompt 1: 'Once upon a time'\n",
            "Generated: Once upon a time,\n",
            "C 10 comingants since,\n",
            " me couldn\n",
            "Iagne this.\n",
            "- my listen\n",
            "she the.\n",
            "I go.\n",
            "I'm not get\n",
            "all the line.\n",
            "But--\n",
            "I'm sorry.\n",
            "I'm not\n",
            "\n",
            "Prompt 2: 'The meaning of life is'\n",
            "Generated: The meaning of life is you\n",
            "Oh trade out.\n",
            "Har'tT do the <.\n",
            "I can I'm is\n",
            "the a a.\n",
            "I'm inclined.\n",
            "I'm not can't--\n",
            "we're my number in.\n",
            "Let's in Jessica's\n",
            "\n",
            "Prompt 3: 'In a galaxy far away'\n",
            "Generated: In a galaxy far awayson someoneCanone\n",
            "that\n",
            "so for a\n",
            "Dothat minutes</.\n",
            "But you me, help bet\n",
            "the like Uh right\n",
            "of theA person\n",
            "in a in scam the way?\n",
            "J on, canuh\n",
            "into,\n",
            "============================================================\n",
            "\n",
            "Step  1000/5000 | Loss: 0.0245 | LR: 5.84e-04 | 49924 tok/s\n",
            "Step  1100/5000 | Loss: 0.0231 | LR: 5.77e-04 | 51848 tok/s\n",
            "Step  1200/5000 | Loss: 0.0191 | LR: 5.68e-04 | 53574 tok/s\n",
            "Step  1300/5000 | Loss: 0.0158 | LR: 5.59e-04 | 55121 tok/s\n",
            "Step  1400/5000 | Loss: 0.0126 | LR: 5.48e-04 | 56515 tok/s\n",
            "Checkpoint saved: checkpoints/checkpoint_step_1500.pt\n",
            "\n",
            "============================================================\n",
            "TEXT GENERATION AT STEP 1500\n",
            "============================================================\n",
            "\n",
            "Prompt 1: 'Once upon a time'\n",
            "Generated: Once upon a time is if against..?\n",
            "You shut adult me reputation with\n",
            "than bit other you said\n",
            "I saw school guy,\n",
            "to'll this to might doing\n",
            "might with suit.\n",
            "If she with of something here.\n",
            "Are you're to\n",
            "\n",
            "Prompt 2: 'The meaning of life is'\n",
            "Generated: The meaning of life is.\n",
            "That never reminds this the trial can\n",
            "what minutes.\n",
            "I wouldn anyway for.\n",
            "J, I what?\n",
            "To're, spoke\n",
            "to go.\n",
            "Hey, Louis made us.\n",
            "I would that see it you do.\n",
            "\n",
            "Prompt 3: 'In a galaxy far away'\n",
            "Generated: In a galaxy far away rather tooone pretty own then in. just DNA telling to.?\n",
            "- happens he\n",
            "-n't ask in the trial the much,\n",
            "how to can fact me.\n",
            "You a second I're\n",
            "not to've me.\n",
            "Okay\n",
            "============================================================\n",
            "\n",
            "Step  1500/5000 | Loss: 0.0168 | LR: 5.37e-04 | 57186 tok/s\n",
            "Step  1600/5000 | Loss: 0.0110 | LR: 5.24e-04 | 58342 tok/s\n",
            "Step  1700/5000 | Loss: 0.0135 | LR: 5.11e-04 | 59424 tok/s\n",
            "Step  1800/5000 | Loss: 0.0075 | LR: 4.96e-04 | 60431 tok/s\n",
            "Step  1900/5000 | Loss: 0.0076 | LR: 4.81e-04 | 61353 tok/s\n",
            "Checkpoint saved: checkpoints/checkpoint_step_2000.pt\n",
            "\n",
            "============================================================\n",
            "TEXT GENERATION AT STEP 2000\n",
            "============================================================\n",
            "\n",
            "Prompt 1: 'Once upon a time'\n",
            "Generated: Once upon a time willIf's am is\n",
            " something playing wasn it\n",
            "what thing.\n",
            "Sorry you can keep it good test,\n",
            "not I did in really.\n",
            "I have my client.\n",
            "But you'll also?\n",
            "Not am on ever\n",
            "a made\n",
            "\n",
            "Prompt 2: 'The meaning of life is'\n",
            "Generated: The meaning of life is yeah\n",
            "every one\n",
            "and for guess.\n",
            "I had to bullshit exactly\n",
            "to hack sixWow.\n",
            "You, she Not happy\n",
            "the twoive.\n",
            "And seemï¿½\n",
            "you did do you?\n",
            "He was wrong.\n",
            "It's\n",
            "\n",
            "Prompt 3: 'In a galaxy far away'\n",
            "Generated: In a galaxy far away can McKhow morning ask my borrow\n",
            "I was to get business deal around\n",
            "Mr Harvey more find?\n",
            "This million lawsuit.\n",
            "That school million when start didn't,.\n",
            "That please son's on.\n",
            "We the...\n",
            "into on\n",
            "============================================================\n",
            "\n",
            "Step  2000/5000 | Loss: 0.0081 | LR: 4.65e-04 | 61714 tok/s\n",
            "Step  2100/5000 | Loss: 0.0043 | LR: 4.48e-04 | 62447 tok/s\n",
            "Step  2200/5000 | Loss: 0.0042 | LR: 4.31e-04 | 63203 tok/s\n",
            "Step  2300/5000 | Loss: 0.0090 | LR: 4.13e-04 | 63907 tok/s\n",
            "Step  2400/5000 | Loss: 0.0029 | LR: 3.95e-04 | 64571 tok/s\n",
            "Checkpoint saved: checkpoints/checkpoint_step_2500.pt\n",
            "\n",
            "============================================================\n",
            "TEXT GENERATION AT STEP 2500\n",
            "============================================================\n",
            "\n",
            "Prompt 1: 'Once upon a time'\n",
            "Generated: Once upon a time man?\n",
            "M of what.\n",
            "How you his some that.\n",
            "That I have to do to\n",
            " myself at the way.\n",
            ".\n",
            "That's what I don't.\n",
            "If you really when-\n",
            "- Good.\n",
            "- Good\n",
            "\n",
            "Prompt 2: 'The meaning of life is'\n",
            "Generated: The meaning of life is.\n",
            "You answeras why before.\n",
            "I got to afford to work.\n",
            "- send's three this've--\n",
            "- It already \" forgetting.\n",
            "always.\n",
            "F when you know how\n",
            "For you lied?\n",
            "to always'm not\n",
            "\n",
            "Prompt 3: 'In a galaxy far away'\n",
            "Generated: In a galaxy far awayOkay alsohere Mike, seat? is.\n",
            "Cal.how against promise\n",
            "whe about work people your.\n",
            "Hey,really!\n",
            "My called out.\n",
            "That's'm not the only wants.\n",
            "You[ does here you call\n",
            "a\n",
            "============================================================\n",
            "\n",
            "Step  2500/5000 | Loss: 0.0024 | LR: 3.77e-04 | 64744 tok/s\n",
            "Step  2600/5000 | Loss: 0.0019 | LR: 3.58e-04 | 65326 tok/s\n",
            "Step  2700/5000 | Loss: 0.0026 | LR: 3.39e-04 | 65884 tok/s\n",
            "Step  2800/5000 | Loss: 0.0030 | LR: 3.21e-04 | 66404 tok/s\n",
            "Step  2900/5000 | Loss: 0.0010 | LR: 3.02e-04 | 66907 tok/s\n",
            "Checkpoint saved: checkpoints/checkpoint_step_3000.pt\n",
            "\n",
            "============================================================\n",
            "TEXT GENERATION AT STEP 3000\n",
            "============================================================\n",
            "\n",
            "Prompt 1: 'Once upon a time'\n",
            "Generated: Once upon a time again,-- with-- set to me\n",
            "- a it..\n",
            "- affair.\n",
            "No...\n",
            "- towel your are anyway.\n",
            "My bullshit do you\n",
            "talking about lieu in another,\n",
            "Yes he me call me corporate\n",
            "any of\n",
            "\n",
            "Prompt 2: 'The meaning of life is'\n",
            "Generated: The meaning of life is Your\n",
            "justDid yesterday there for.\n",
            "Harvey still Trevor.\n",
            "And I did this it\n",
            "in,\n",
            " will we you can find?\n",
            "\", yeah,\n",
            "but I did to believe you fast...\n",
            "You'reGonnaeller\n",
            "\n",
            "Prompt 3: 'In a galaxy far away'\n",
            "Generated: In a galaxy far away give.\n",
            "Hey our.\n",
            "==-- a friend\n",
            "and my senior.\n",
            "We wants Inv't be call?\n",
            "F the've the only.\n",
            "M asshole's?\n",
            "Does not can\n",
            "with up, Eleanor also\n",
            "who retie\n",
            "============================================================\n",
            "\n",
            "Step  3000/5000 | Loss: 0.0019 | LR: 2.83e-04 | 66976 tok/s\n",
            "Step  3100/5000 | Loss: 0.0015 | LR: 2.65e-04 | 67414 tok/s\n",
            "Step  3200/5000 | Loss: 0.0040 | LR: 2.47e-04 | 67843 tok/s\n",
            "Step  3300/5000 | Loss: 0.0008 | LR: 2.29e-04 | 68206 tok/s\n",
            "Step  3400/5000 | Loss: 0.0011 | LR: 2.12e-04 | 68593 tok/s\n",
            "Checkpoint saved: checkpoints/checkpoint_step_3500.pt\n",
            "\n",
            "============================================================\n",
            "TEXT GENERATION AT STEP 3500\n",
            "============================================================\n",
            "\n",
            "Prompt 1: 'Once upon a time'\n",
            "Generated: Once upon a time this\n",
            "youW he soundAre you inone,--'s\n",
            "to to stuff, leftrun.\n",
            "You're been pretend a\n",
            "of to take your2.\n",
            "S,'s,'s\n",
            "is to do that.\n",
            "It... if\n",
            "\n",
            "Prompt 2: 'The meaning of life is'\n",
            "Generated: The meaning of life is tomorrow mean IHar is did\n",
            "thatno\n",
            "just have anymore.\n",
            "I be aloneBall\n",
            " mat\n",
            "the I read you\n",
            " her hisrank,\n",
            "off.\n",
            "What if,\n",
            "but scathing andog,\n",
            "your first alone\n",
            "for\n",
            "\n",
            "Prompt 3: 'In a galaxy far away'\n",
            "Generated: In a galaxy far awayL believe, happens,vel sure\n",
            "'re claim theAll made her something,\n",
            "harLouis anyway for.\n",
            " out your go,wethe actually about.\n",
            "We whole is our reason\n",
            "all thing I can.\n",
            "- had here health.\n",
            "============================================================\n",
            "\n",
            "Step  3500/5000 | Loss: 0.0005 | LR: 1.95e-04 | 68604 tok/s\n",
            "Step  3600/5000 | Loss: 0.0041 | LR: 1.79e-04 | 68950 tok/s\n",
            "Step  3700/5000 | Loss: 0.0008 | LR: 1.64e-04 | 69294 tok/s\n",
            "Step  3800/5000 | Loss: 0.0008 | LR: 1.49e-04 | 69621 tok/s\n",
            "Step  3900/5000 | Loss: 0.0007 | LR: 1.36e-04 | 69929 tok/s\n",
            "Checkpoint saved: checkpoints/checkpoint_step_4000.pt\n",
            "\n",
            "============================================================\n",
            "TEXT GENERATION AT STEP 4000\n",
            "============================================================\n",
            "\n",
            "Prompt 1: 'Once upon a time'\n",
            "Generated: Once upon a time might ruined?\n",
            "-\n",
            "- I, youA when you.\n",
            "- some I'm said on an?\n",
            "- Wow.\n",
            "I there's the way.\n",
            "- the way\n",
            "to side of\n",
            "really cases they document,\n",
            "or\n",
            "\n",
            "Prompt 2: 'The meaning of life is'\n",
            "Generated: The meaning of life is.\n",
            "He trusting expect is.\n",
            "I mean.\n",
            "That need Maz music up office night?\n",
            "Jessica deny.\n",
            "That know you can be me\n",
            "you come to 1 you You?\n",
            "Oh, are you know why\n",
            "to's not\n",
            "\n",
            "Prompt 3: 'In a galaxy far away'\n",
            "Generated: In a galaxy far awaytold with in, my much. for\n",
            "and my say, I plan\n",
            "We let her--\n",
            " to to your\n",
            "the the same search.\n",
            "I do you can can\n",
            "to exist that Mike.\n",
            "They have access any,\n",
            "we\n",
            "============================================================\n",
            "\n",
            "Step  4000/5000 | Loss: 0.0007 | LR: 1.23e-04 | 69896 tok/s\n",
            "Step  4100/5000 | Loss: 0.0004 | LR: 1.12e-04 | 70178 tok/s\n",
            "Step  4200/5000 | Loss: 0.0006 | LR: 1.01e-04 | 70457 tok/s\n",
            "Step  4300/5000 | Loss: 0.0007 | LR: 9.16e-05 | 70731 tok/s\n",
            "Step  4400/5000 | Loss: 0.0004 | LR: 8.33e-05 | 70993 tok/s\n",
            "Checkpoint saved: checkpoints/checkpoint_step_4500.pt\n",
            "\n",
            "============================================================\n",
            "TEXT GENERATION AT STEP 4500\n",
            "============================================================\n",
            "\n",
            "Prompt 1: 'Once upon a time'\n",
            "Generated: Once upon a time many with behind me looked. I so find\n",
            " a responsible innocent\n",
            "and a lawsuit knew.\n",
            "Not don't want\n",
            "what how you.\n",
            "-.\n",
            "- L like in your lawyer\n",
            "for her.\n",
            "They don't take what?\n",
            "\n",
            "Prompt 2: 'The meaning of life is'\n",
            "Generated: The meaning of life is.\n",
            "Heer report.\n",
            "I likelo street drug court.\n",
            "If you asshole,\n",
            "just and did away.\n",
            "[Cleff you's\n",
            "to count a books red.\n",
            "And is the facts.\n",
            "[dist, I guess\n",
            "\n",
            "Prompt 3: 'In a galaxy far away'\n",
            "Generated: In a galaxy far awaytold withoffic see now with out you.\n",
            "Callrelations\n",
            "you before when your civil\n",
            "We have,\n",
            "the\n",
            "Sorry our hadn\n",
            " Look scenario me for\n",
            "in the called time,\n",
            "so they know missed.\n",
            "And after.\n",
            "\n",
            "============================================================\n",
            "\n",
            "Step  4500/5000 | Loss: 0.0006 | LR: 7.63e-05 | 70912 tok/s\n",
            "Step  4600/5000 | Loss: 0.0007 | LR: 7.05e-05 | 71150 tok/s\n",
            "Step  4700/5000 | Loss: 0.0004 | LR: 6.59e-05 | 71388 tok/s\n",
            "Step  4800/5000 | Loss: 0.0006 | LR: 6.26e-05 | 71620 tok/s\n",
            "Step  4900/5000 | Loss: 0.0007 | LR: 6.07e-05 | 71846 tok/s\n",
            "Step  4999/5000 | Loss: 0.0006 | LR: 6.00e-05 | 72056 tok/s\n",
            "Checkpoint saved: checkpoints/checkpoint_step_5000.pt\n",
            "\n",
            "============================================================\n",
            "TEXT GENERATION AT STEP 5000\n",
            "============================================================\n",
            "\n",
            "Prompt 1: 'Once upon a time'\n",
            "Generated: Once upon a time wearing., want did actuallyiful,and\n",
            "I meanwell for\n",
            "might pretend\n",
            "I a temple,\n",
            "[S a bad?\n",
            "Nobody, I not what I&\n",
            "see you bail the warn,\n",
            "'re a I'm email\n",
            "\n",
            "\n",
            "Prompt 2: 'The meaning of life is'\n",
            "Generated: The meaning of life is, times believe\n",
            " attached speaking was by associate,\n",
            "off's 9 hearing\n",
            "this I\n",
            " been changed fired\n",
            " theBoy's-inner.\n",
            "- Good.\n",
            "- able and four\n",
            " bo So you do us.\n",
            "You think me of\n",
            "\n",
            "Prompt 3: 'In a galaxy far away'\n",
            "Generated: In a galaxy far away hasCh with\n",
            " on me door\n",
            "any a in you as defense.\n",
            "Michael you can.\n",
            "But I gave not\n",
            "to exact same--\n",
            "I you took every.\n",
            "Um and I have\n",
            "the warrant,\n",
            "and,'t's\n",
            "============================================================\n",
            "\n",
            "\n",
            "============================================================\n",
            "TRAINING COMPLETE\n",
            "============================================================\n",
            "Final loss: 0.0006\n",
            "Total time: 1141.2s (19.0 minutes)\n",
            "Average tokens/sec: 71786\n"
          ]
        }
      ],
      "source": [
        "for step in range(max_steps):\n",
        "    # Update learning rate\n",
        "    lr = get_lr(step)\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = lr\n",
        "\n",
        "    # Gradient accumulation\n",
        "    optimizer.zero_grad()\n",
        "    loss_accum = 0.0\n",
        "\n",
        "    for micro_step in range(gradient_accumulation_steps):\n",
        "        x, y = train_loader.next_batch()\n",
        "        x, y = x.to(device), y.to(device)\n",
        "\n",
        "        # Speedup: Mixed precision (bfloat16)\n",
        "        with torch.autocast(device_type=device, dtype=torch.bfloat16):\n",
        "            logits, loss = model(x, y)\n",
        "\n",
        "        loss = loss / gradient_accumulation_steps\n",
        "        loss_accum += loss.detach()\n",
        "        loss.backward()\n",
        "\n",
        "    # Gradient clipping\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=grad_clip)\n",
        "\n",
        "    # Optimizer step\n",
        "    optimizer.step()\n",
        "\n",
        "    # Record loss\n",
        "    loss_history.append(loss_accum.item())\n",
        "\n",
        "    # Logging\n",
        "    if step % 100 == 0 or step == max_steps - 1:\n",
        "        elapsed = time.time() - start_time\n",
        "        tokens_per_sec = (step + 1) * total_batch_size * sequence_length / elapsed\n",
        "        print(f'Step {step:5d}/{max_steps} | Loss: {loss_accum.item():.4f} | LR: {lr:.2e} | {tokens_per_sec:.0f} tok/s')\n",
        "\n",
        "    # Checkpoint and generate text every checkpoint_interval steps\n",
        "    if (step + 1) % checkpoint_interval == 0:\n",
        "        # Save checkpoint\n",
        "        checkpoint_path = os.path.join(checkpoint_dir, f'checkpoint_step_{step+1}.pt')\n",
        "        save_checkpoint(model, optimizer, step + 1, loss_accum.item(), checkpoint_path)\n",
        "\n",
        "        # Generate text samples\n",
        "        generate_samples(model, step + 1, device)\n",
        "\n",
        "# Final statistics\n",
        "elapsed = time.time() - start_time\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"TRAINING COMPLETE\")\n",
        "print(\"=\"*60)\n",
        "print(f'Final loss: {loss_accum.item():.4f}')\n",
        "print(f'Total time: {elapsed:.1f}s ({elapsed/60:.1f} minutes)')\n",
        "print(f'Average tokens/sec: {max_steps * total_batch_size * sequence_length / elapsed:.0f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "px63ob4uVM4s"
      },
      "source": [
        "## 11. Visualize Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 494
        },
        "id": "l9pUTK1QVM4s",
        "outputId": "fdec3212-bc45-445e-fe61-a1ac069459c9"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1200x400 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Loss Statistics:\n",
            "  Starting loss: 11.1658\n",
            "  Final loss: 0.0006\n",
            "  Min loss: 0.0001\n"
          ]
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(12, 4))\n",
        "\n",
        "# Loss curve\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(loss_history)\n",
        "plt.xlabel('Step')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training Loss')\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Loss curve (log scale)\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(loss_history)\n",
        "plt.xlabel('Step')\n",
        "plt.ylabel('Loss')\n",
        "plt.yscale('log')\n",
        "plt.title('Training Loss (Log Scale)')\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('training_loss.png', dpi=150)\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\nLoss Statistics:\")\n",
        "print(f\"  Starting loss: {loss_history[0]:.4f}\")\n",
        "print(f\"  Final loss: {loss_history[-1]:.4f}\")\n",
        "print(f\"  Min loss: {min(loss_history):.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I3NVqWXEVM4s"
      },
      "source": [
        "## 12. STOP - Training Complete at 5000 Steps\n",
        "\n",
        "**Now we will simulate stopping and resuming from checkpoint.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r7Kp7qRQVM4s",
        "outputId": "f7490808-5597-4e99-8439-6d957720955a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "TRAINING STOPPED AT STEP 5000\n",
            "Simulating complete stop...\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "print(\"=\"*60)\n",
        "print(\"TRAINING STOPPED AT STEP 5000\")\n",
        "print(\"Simulating complete stop...\")\n",
        "print(\"=\"*60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JGlrC_0mVM4s"
      },
      "source": [
        "## 13. Resume Training from Checkpoint\n",
        "\n",
        "**Critical step for assignment**: Load checkpoint and continue training for 50 more steps."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2FyOn-CwVM4s",
        "outputId": "88e3ac51-12a1-42ef-f83d-907e9f5022fb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "CREATING FRESH MODEL (simulating restart)\n",
            "============================================================\n",
            "Fresh model created.\n"
          ]
        }
      ],
      "source": [
        "# Create a fresh model and optimizer (simulating restart)\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"CREATING FRESH MODEL (simulating restart)\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "config_fresh = SmolLM2Config()\n",
        "model_fresh = SmolLM2(config_fresh)\n",
        "model_fresh.to(device)\n",
        "\n",
        "# Compile if available\n",
        "if device == 'cuda' and hasattr(torch, 'compile'):\n",
        "    model_fresh = torch.compile(model_fresh)\n",
        "\n",
        "optimizer_fresh = torch.optim.AdamW(\n",
        "    model_fresh.parameters(),\n",
        "    lr=max_lr,\n",
        "    betas=(0.9, 0.95),\n",
        "    eps=1e-8,\n",
        "    weight_decay=weight_decay,\n",
        "    fused=True if device == 'cuda' else False\n",
        ")\n",
        "\n",
        "print(\"Fresh model created.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ud3_pXXrVM4s",
        "outputId": "1ca83b35-6a27-415e-d8d9-66e2923866fc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "LOADING CHECKPOINT FROM STEP 5000\n",
            "============================================================\n",
            "Checkpoint loaded: checkpoints/checkpoint_step_5000.pt (step 5000, loss 0.0006)\n"
          ]
        }
      ],
      "source": [
        "# Load checkpoint from step 5000\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"LOADING CHECKPOINT FROM STEP 5000\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "checkpoint_path = os.path.join(checkpoint_dir, 'checkpoint_step_5000.pt')\n",
        "resume_step, resume_loss = load_checkpoint(model_fresh, optimizer_fresh, checkpoint_path, device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sgd0DRdNVM4s",
        "outputId": "52da8555-722c-4154-9ab8-0448981c06ce"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "RESUMING TRAINING FROM STEP 5000 FOR 50 MORE STEPS\n",
            "============================================================\n",
            "\n",
            "Step  5001/5050 | Loss: 0.0004 | LR: 6.00e-05\n",
            "Step  5002/5050 | Loss: 0.0005 | LR: 6.00e-05\n",
            "Step  5003/5050 | Loss: 0.0005 | LR: 6.00e-05\n",
            "Step  5004/5050 | Loss: 0.0007 | LR: 6.00e-05\n",
            "Step  5005/5050 | Loss: 0.0007 | LR: 6.00e-05\n",
            "Step  5006/5050 | Loss: 0.0007 | LR: 6.00e-05\n",
            "Step  5007/5050 | Loss: 0.0009 | LR: 6.00e-05\n",
            "Step  5008/5050 | Loss: 0.0001 | LR: 6.00e-05\n",
            "Step  5009/5050 | Loss: 0.0005 | LR: 6.00e-05\n",
            "Step  5010/5050 | Loss: 0.0007 | LR: 6.00e-05\n",
            "Step  5011/5050 | Loss: 0.0006 | LR: 6.00e-05\n",
            "Step  5012/5050 | Loss: 0.0007 | LR: 6.00e-05\n",
            "Step  5013/5050 | Loss: 0.0005 | LR: 6.00e-05\n",
            "Step  5014/5050 | Loss: 0.0010 | LR: 6.00e-05\n",
            "Step  5015/5050 | Loss: 0.0006 | LR: 6.00e-05\n",
            "Step  5016/5050 | Loss: 0.0004 | LR: 6.00e-05\n",
            "Step  5017/5050 | Loss: 0.0005 | LR: 6.00e-05\n",
            "Step  5018/5050 | Loss: 0.0005 | LR: 6.00e-05\n",
            "Step  5019/5050 | Loss: 0.0007 | LR: 6.00e-05\n",
            "Step  5020/5050 | Loss: 0.0007 | LR: 6.00e-05\n",
            "Step  5021/5050 | Loss: 0.0007 | LR: 6.00e-05\n",
            "Step  5022/5050 | Loss: 0.0009 | LR: 6.00e-05\n",
            "Step  5023/5050 | Loss: 0.0001 | LR: 6.00e-05\n",
            "Step  5024/5050 | Loss: 0.0005 | LR: 6.00e-05\n",
            "Step  5025/5050 | Loss: 0.0007 | LR: 6.00e-05\n",
            "Step  5026/5050 | Loss: 0.0006 | LR: 6.00e-05\n",
            "Step  5027/5050 | Loss: 0.0007 | LR: 6.00e-05\n",
            "Step  5028/5050 | Loss: 0.0005 | LR: 6.00e-05\n",
            "Step  5029/5050 | Loss: 0.0010 | LR: 6.00e-05\n",
            "Step  5030/5050 | Loss: 0.0006 | LR: 6.00e-05\n",
            "Step  5031/5050 | Loss: 0.0004 | LR: 6.00e-05\n",
            "Step  5032/5050 | Loss: 0.0005 | LR: 6.00e-05\n",
            "Step  5033/5050 | Loss: 0.0005 | LR: 6.00e-05\n",
            "Step  5034/5050 | Loss: 0.0007 | LR: 6.00e-05\n",
            "Step  5035/5050 | Loss: 0.0007 | LR: 6.00e-05\n",
            "Step  5036/5050 | Loss: 0.0007 | LR: 6.00e-05\n",
            "Step  5037/5050 | Loss: 0.0009 | LR: 6.00e-05\n",
            "Step  5038/5050 | Loss: 0.0001 | LR: 6.00e-05\n",
            "Step  5039/5050 | Loss: 0.0005 | LR: 6.00e-05\n",
            "Step  5040/5050 | Loss: 0.0007 | LR: 6.00e-05\n",
            "Step  5041/5050 | Loss: 0.0006 | LR: 6.00e-05\n",
            "Step  5042/5050 | Loss: 0.0007 | LR: 6.00e-05\n",
            "Step  5043/5050 | Loss: 0.0005 | LR: 6.00e-05\n",
            "Step  5044/5050 | Loss: 0.0010 | LR: 6.00e-05\n",
            "Step  5045/5050 | Loss: 0.0006 | LR: 6.00e-05\n",
            "Step  5046/5050 | Loss: 0.0004 | LR: 6.00e-05\n",
            "Step  5047/5050 | Loss: 0.0005 | LR: 6.00e-05\n",
            "Step  5048/5050 | Loss: 0.0005 | LR: 6.00e-05\n",
            "Step  5049/5050 | Loss: 0.0007 | LR: 6.00e-05\n",
            "Step  5050/5050 | Loss: 0.0007 | LR: 6.00e-05\n",
            "Checkpoint saved: checkpoints/checkpoint_step_5050.pt\n",
            "\n",
            "============================================================\n",
            "TEXT GENERATION AT STEP 5050\n",
            "============================================================\n",
            "\n",
            "Prompt 1: 'Once upon a time'\n",
            "Generated: Once upon a time ', with\n",
            " me thing services.\n",
            "This face to meet we off to.\n",
            "We-- for, I can you've been says.\n",
            "- Harvey the way, you would about me\n",
            "- I'm make for.\n",
            "- And I\n",
            "\n",
            "Prompt 2: 'The meaning of life is'\n",
            "Generated: The meaning of life is.\n",
            "- employees: up.\n",
            "- twins.\n",
            "Would corporate see?\n",
            "- No.\n",
            "You're hold seem.\n",
            "- No on.\n",
            "-size.\n",
            "- What was you.\n",
            "Listen time that phone.\n",
            "And I\n",
            "\n",
            "Prompt 3: 'In a galaxy far away'\n",
            "Generated: In a galaxy far away afterone with fine so Jerome stuff re today\n",
            "before didn't business, right. life\n",
            "every you about the'm\n",
            " whatsoever just where where apart.\n",
            "It'reabout, this.\n",
            "We-- They, much of\n",
            "me bill.\n",
            "\n",
            "============================================================\n",
            "\n",
            "\n",
            "============================================================\n",
            "RESUMED TRAINING COMPLETE\n",
            "============================================================\n",
            "Trained from step 5000 to step 5050\n",
            "Final loss: 0.0007\n",
            "Time for 50 steps: 14.0s\n"
          ]
        }
      ],
      "source": [
        "# Continue training for 50 more steps\n",
        "additional_steps = 50\n",
        "total_steps_after_resume = resume_step + additional_steps\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(f\"RESUMING TRAINING FROM STEP {resume_step} FOR {additional_steps} MORE STEPS\")\n",
        "print(\"=\"*60 + \"\\n\")\n",
        "\n",
        "loss_history_resumed = []\n",
        "start_time_resumed = time.time()\n",
        "\n",
        "for step in range(resume_step, total_steps_after_resume):\n",
        "    # Update learning rate (continuing the schedule)\n",
        "    lr = get_lr(step)\n",
        "    for param_group in optimizer_fresh.param_groups:\n",
        "        param_group['lr'] = lr\n",
        "\n",
        "    # Gradient accumulation\n",
        "    optimizer_fresh.zero_grad()\n",
        "    loss_accum = 0.0\n",
        "\n",
        "    for micro_step in range(gradient_accumulation_steps):\n",
        "        x, y = train_loader.next_batch()\n",
        "        x, y = x.to(device), y.to(device)\n",
        "\n",
        "        with torch.autocast(device_type=device, dtype=torch.bfloat16):\n",
        "            logits, loss = model_fresh(x, y)\n",
        "\n",
        "        loss = loss / gradient_accumulation_steps\n",
        "        loss_accum += loss.detach()\n",
        "        loss.backward()\n",
        "\n",
        "    # Gradient clipping\n",
        "    torch.nn.utils.clip_grad_norm_(model_fresh.parameters(), max_norm=grad_clip)\n",
        "\n",
        "    # Optimizer step\n",
        "    optimizer_fresh.step()\n",
        "\n",
        "    # Record loss\n",
        "    loss_history_resumed.append(loss_accum.item())\n",
        "\n",
        "    # Log every step for the resumed training\n",
        "    print(f'Step {step + 1:5d}/{total_steps_after_resume} | Loss: {loss_accum.item():.4f} | LR: {lr:.2e}')\n",
        "\n",
        "# Save final checkpoint\n",
        "final_checkpoint_path = os.path.join(checkpoint_dir, f'checkpoint_step_{total_steps_after_resume}.pt')\n",
        "save_checkpoint(model_fresh, optimizer_fresh, total_steps_after_resume, loss_accum.item(), final_checkpoint_path)\n",
        "\n",
        "# Generate final samples\n",
        "generate_samples(model_fresh, total_steps_after_resume, device)\n",
        "\n",
        "elapsed_resumed = time.time() - start_time_resumed\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(f\"RESUMED TRAINING COMPLETE\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Trained from step {resume_step} to step {total_steps_after_resume}\")\n",
        "print(f\"Final loss: {loss_accum.item():.4f}\")\n",
        "print(f\"Time for {additional_steps} steps: {elapsed_resumed:.1f}s\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hAgQaA50VM4s"
      },
      "source": [
        "## 14. Final Summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1unQg6RoVM4s",
        "outputId": "b35a3fb2-4d9f-4888-a421-69ca501ae66e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "TRAINING SUMMARY\n",
            "============================================================\n",
            "\n",
            "Model: SmolLM2-135M\n",
            "Total Parameters: 135,178,560\n",
            "\n",
            "Training:\n",
            "  Initial training: 5000 steps\n",
            "  Resumed training: 50 steps (from checkpoint)\n",
            "  Total steps: 5050\n",
            "\n",
            "Checkpoints saved:\n",
            "  - checkpoint_step_1000.pt\n",
            "  - checkpoint_step_1500.pt\n",
            "  - checkpoint_step_2000.pt\n",
            "  - checkpoint_step_2500.pt\n",
            "  - checkpoint_step_3000.pt\n",
            "  - checkpoint_step_3500.pt\n",
            "  - checkpoint_step_4000.pt\n",
            "  - checkpoint_step_4500.pt\n",
            "  - checkpoint_step_500.pt\n",
            "  - checkpoint_step_5000.pt\n",
            "  - checkpoint_step_5050.pt\n",
            "\n",
            "Speedups used:\n",
            "  - FlashAttention (scaled_dot_product_attention)\n",
            "  - Mixed precision (bfloat16)\n",
            "  - torch.compile (if available)\n",
            "  - TF32 precision\n",
            "  - Gradient accumulation (4x)\n",
            "  - Fused AdamW optimizer\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"TRAINING SUMMARY\")\n",
        "print(\"=\"*60)\n",
        "print(f\"\\nModel: SmolLM2-135M\")\n",
        "print(f\"Total Parameters: {total_params:,}\")\n",
        "print(f\"\\nTraining:\")\n",
        "print(f\"  Initial training: 5000 steps\")\n",
        "print(f\"  Resumed training: 50 steps (from checkpoint)\")\n",
        "print(f\"  Total steps: 5050\")\n",
        "print(f\"\\nCheckpoints saved:\")\n",
        "for f in sorted(os.listdir(checkpoint_dir)):\n",
        "    print(f\"  - {f}\")\n",
        "print(f\"\\nSpeedups used:\")\n",
        "print(f\"  - FlashAttention (scaled_dot_product_attention)\")\n",
        "print(f\"  - Mixed precision (bfloat16)\")\n",
        "print(f\"  - torch.compile (if available)\")\n",
        "print(f\"  - TF32 precision\")\n",
        "print(f\"  - Gradient accumulation ({gradient_accumulation_steps}x)\")\n",
        "print(f\"  - Fused AdamW optimizer\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9oaPSiaAVM4s"
      },
      "source": [
        "## 15. Save Model for HuggingFace Spaces"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-pCFDFskVM4t",
        "outputId": "75f6f7e3-f083-4a60-f77d-8da760ac49a3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Final model saved to 'smollm2_135m_final.pt'\n"
          ]
        }
      ],
      "source": [
        "# Save final model\n",
        "model_to_save = model_fresh._orig_mod if hasattr(model_fresh, '_orig_mod') else model_fresh\n",
        "torch.save({\n",
        "    'model_state_dict': model_to_save.state_dict(),\n",
        "    'config': config,\n",
        "}, 'smollm2_135m_final.pt')\n",
        "print(\"Final model saved to 'smollm2_135m_final.pt'\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
